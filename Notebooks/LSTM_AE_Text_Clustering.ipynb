{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "from string import punctuation\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from pickle import load\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import csv, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/admin-karim/Downloads/BiB_2019_Clustering_Paper/drugsCom_raw/drugsComTrain_raw.tsv', sep='\\t')\n",
    "df_test = pd.read_csv('C:/Users/admin-karim/Downloads/BiB_2019_Clustering_Paper/drugsCom_raw/drugsComTest_raw.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>206461</td>\n",
       "      <td>Valsartan</td>\n",
       "      <td>Left Ventricular Dysfunction</td>\n",
       "      <td>\"It has no side effect, I take it in combinati...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>May 20, 2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>Birth Control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>Opiate Dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  drugName                     condition  \\\n",
       "0      206461                 Valsartan  Left Ventricular Dysfunction   \n",
       "1       95260                Guanfacine                          ADHD   \n",
       "2       92703                    Lybrel                 Birth Control   \n",
       "3      138000                Ortho Evra                 Birth Control   \n",
       "4       35696  Buprenorphine / naloxone             Opiate Dependence   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"It has no side effect, I take it in combinati...     9.0   \n",
       "1  \"My son is halfway through his fourth week of ...     8.0   \n",
       "2  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "3  \"This is my first time using any form of birth...     8.0   \n",
       "4  \"Suboxone has completely turned my life around...     9.0   \n",
       "\n",
       "                date  usefulCount  \n",
       "0       May 20, 2012           27  \n",
       "1     April 27, 2010          192  \n",
       "2  December 14, 2009           17  \n",
       "3   November 3, 2015           10  \n",
       "4  November 27, 2016           37  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     215063\n",
       "drugName       215063\n",
       "condition      213869\n",
       "review         215063\n",
       "rating         215063\n",
       "date           215063\n",
       "usefulCount    215063\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_new = df_full[['review','rating']]\n",
    "#df_full_new['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_full_new.loc[df_full_new.rating <= 4, 'rating'] = 0\n",
    "df_full_new.loc[df_full_new.rating > 7, 'rating'] = 2\n",
    "df_full_new.loc[df_full_new.rating >= 5, 'rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "reviews = df_full_new['review'].values\n",
    "y = df_full_new['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150544, 53558)\n",
      "Done transforming\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(reviews)\n",
    "X = vectorizer.transform(reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print('Done transforming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting\n",
      "Done dumping\n",
      "Done loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "print('Done fitting')\n",
    "\n",
    "dump(classifier, 'classifier')\n",
    "print('Done dumping')\n",
    "\n",
    "classifier = load('classifier')\n",
    "y_pred = classifier.predict(X_test)\n",
    "print('Done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.70      0.71     16272\n",
      "         1.0       0.15      0.49      0.22      2918\n",
      "         2.0       0.92      0.79      0.85     45329\n",
      "\n",
      "    accuracy                           0.75     64519\n",
      "   macro avg       0.59      0.66      0.59     64519\n",
      "weighted avg       0.83      0.75      0.79     64519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nClassification report:\\n', classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "en_model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/admin-karim/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model, words):\n",
    "    #en_model = Word2Vec.load_word2vec_format('C:/Users/admin-karim/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True, norm_only=True)\n",
    "    #en_model = ft.load_fasttext_format('C:/Users/admin-karim/Downloads/GoogleNews-vectors-negative300.bin.gz')\n",
    "    embedding = np.array([])\n",
    "    for word in words:\n",
    "        embedding = np.append(embedding, model[word[0]])\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, n_grams):\n",
    "    \"\"\"\n",
    "    remove punctuation and non-alpha characters, return all tokens after lower case-ing and splitting by space\n",
    "    :param n_grams: length of n-gram\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return [' '.join(x) for x in ngrams(tokens, n_grams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    output = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        output.append(tokens[i:i + n])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(lines, num_words, n_grams):\n",
    "    all_tokens = []\n",
    "    for line in lines:\n",
    "        tokens = tokenize(line, n_grams)\n",
    "        for t in tokens:\n",
    "            if t not in stopwords:\n",
    "                all_tokens.append(t)\n",
    "    counter = Counter(all_tokens)\n",
    "    return counter.most_common(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    for i, sentence in enumerate(train_sentences):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        train_sentences[i] = ' '.join(new_sent)\n",
    "    return train_sentences\n",
    "\n",
    "train_sentences = filter_stop_words(reviews, stop_words)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(tokenizer, reviews, length):\n",
    "    #print(lines)\n",
    "    encoded=tokenizer.texts_to_sequences(reviews)\n",
    "    print(encoded[0:10])\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 53636\n",
      "\n",
      "[[3, 7, 137, 1, 16, 790, 2936, 49, 91, 3843, 1552], [15, 589, 3394, 1628, 33, 3326, 460, 345, 1190, 312, 89, 33, 19, 8, 2347, 90, 223, 232, 59, 23, 95, 899, 22, 266, 3523, 732, 591, 159, 87, 218, 448, 455, 2758, 369, 2075, 1824, 1, 593, 35, 1392, 144, 200, 730, 23, 150, 455, 99, 144, 17, 89, 59, 23, 233, 325, 315, 51, 14269, 111, 315, 188, 479, 253, 65, 151, 188, 3523, 315, 2164, 208, 1913, 334, 1947, 54, 460, 75, 155, 181, 356, 108, 242], [1, 94, 16, 206, 1167, 1816, 860, 30, 482, 193, 235, 113, 1875, 49, 23, 7, 13, 97, 6363, 1262, 28300, 1150, 938, 1, 280, 9092, 3124, 1076, 236, 139, 507, 1, 19, 9092, 359, 14, 5, 37, 2001, 200, 179, 37, 402, 59, 32, 236, 8, 201, 416, 59, 32, 179, 57, 515, 416, 208, 40, 68, 140, 515, 37, 402, 59, 32, 3, 2, 10, 367, 515, 33, 1, 47, 192, 1441, 887, 17, 410, 7, 1, 66, 2, 4, 7, 13, 17, 936, 37, 325, 10618, 7653], [28, 14, 24, 122, 687, 81, 63, 1, 2, 6, 542, 1, 55, 389, 1, 159, 18, 285, 14, 3, 664, 765, 924, 17, 1161, 83, 113, 249, 49, 76, 23, 1901, 1, 94, 113, 25, 53, 23, 1875, 36, 83, 119, 645, 14, 59, 23, 37, 1, 52, 119, 122, 81, 63, 630, 193, 389], [846, 205, 751, 43, 157, 1, 26, 2240, 1, 2, 6, 20524, 399, 1, 152, 877, 2084, 3882, 5627, 1, 678, 846, 1090, 11, 4753, 1396, 15, 16110, 320, 1090, 24, 1, 40, 1, 19, 10842, 25533, 9093, 16793, 1331, 105, 1350, 199, 172, 2, 477, 841, 143, 272, 2, 10, 65, 1085, 846, 100, 4671, 69, 43, 134, 1, 186, 7, 13, 747, 942, 1396, 1, 2, 6, 251, 393, 54, 424, 438, 42, 3, 753, 318, 17, 606, 22037, 3428, 1, 1090, 1396], [581, 5, 474, 19, 58, 2500, 306, 2232, 141, 16794, 286, 379, 675, 16795, 604, 5, 2232, 27, 411, 107, 427, 651, 1917, 769, 98, 379, 675, 16795, 21, 204, 71, 20, 132, 2, 4, 2820, 1006, 28301, 2232, 7, 13, 4912, 18, 1671, 1, 2, 6, 418, 107, 58, 8507, 33, 146, 222, 7, 13], [315, 1817, 215, 16796, 226, 42, 1, 34, 387, 487, 1280, 87, 176, 34, 396, 647, 59, 32, 176, 1, 2, 6, 259], [974, 273, 43, 272, 258, 1, 398, 2280, 1, 14, 19, 974, 551, 248, 398, 84, 12596, 1473, 876, 2062, 15, 987, 63, 1, 293, 2448, 31, 201, 982, 3327, 287, 2062, 154, 15, 243, 92, 1428, 974, 327, 1, 26, 21, 1, 1507, 187, 1, 126, 1, 2, 6, 146, 2211, 2416, 377, 383, 3060, 1, 126, 974, 78, 1, 75, 22, 128, 876, 63, 1023, 55, 29, 1, 7717, 134, 1, 474, 1136, 192, 1, 860, 54, 1, 111, 246, 354, 7, 137, 1, 21, 180, 116], [1, 9, 110, 190, 17820, 383, 1434, 969, 107, 510, 107, 4082, 107, 2391, 103, 969, 107, 510, 98, 235, 4083, 245, 107, 361], [1, 30, 155, 11, 236, 35, 273, 1659, 3816, 242, 3, 50, 102, 205, 1356, 79, 461, 76, 18, 187, 1, 131, 907, 39, 2263, 479, 677, 263, 1, 118, 8, 463, 1, 19, 122, 1318, 1079, 81, 63, 19, 16, 463, 1, 658, 79, 262, 29, 551, 998, 1, 50, 258, 123, 21, 84, 39, 131, 1113, 988, 1, 1906, 57, 1, 2, 6, 36, 2340, 558, 229, 3383, 208, 1, 338, 897, 11995, 247, 36, 1, 258, 268, 2, 4, 1927, 1459, 1, 412, 1, 293, 638, 3, 2, 296, 21, 1, 2469]]\n",
      "(215063, 300)\n"
     ]
    }
   ],
   "source": [
    "trainLength=300\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print()\n",
    "X_train = encode_text(tokenizer, reviews, trainLength)\n",
    "print(X_train.shape)\n",
    "#trainX[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Computing embedding\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a3675d18faf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# 2D embedding of the digits dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Computing embedding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mX_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpectralEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding_.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \"\"\"\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    519\u001b[0m                               \"name or a callable. Got: %s\") % self.affinity)\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m         \u001b[0maffinity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_affinity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         self.embedding_ = spectral_embedding(affinity_matrix,\n\u001b[0;32m    523\u001b[0m                                              \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\manifold\\spectral_embedding_.py\u001b[0m in \u001b[0;36m_get_affinity_matrix\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    473\u001b[0m                 self.affinity_matrix_ = kneighbors_graph(X, self.n_neighbors_,\n\u001b[0;32m    474\u001b[0m                                                          \u001b[0minclude_self\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m                                                          n_jobs=self.n_jobs)\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[1;31m# currently only symmetric affinity_matrix supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 self.affinity_matrix_ = 0.5 * (self.affinity_matrix_ +\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neighbors\\graph.py\u001b[0m in \u001b[0;36mkneighbors_graph\u001b[1;34m(X, n_neighbors, mode, metric, p, metric_params, include_self, n_jobs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_query_include_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_self\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors_graph\u001b[1;34m(self, X, n_neighbors, mode)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'connectivity'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0mA_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mA_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'distance'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 delayed_query(\n\u001b[0;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m             )\n\u001b[0;32m    456\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[1;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m--> 291\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\neighbors\\binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.kd_tree.BinaryTree.query\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\neighbors\\binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.kd_tree.NeighborsHeap.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Authors: Gael Varoquaux\n",
    "# License: BSD 3 clause (C) INRIA 2014\n",
    "\n",
    "print(__doc__)\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "X = X_train\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Visualize the clustering\n",
    "def plot_clustering(X_red, labels, title=None):\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for i in range(X_red.shape[0]):\n",
    "        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),\n",
    "                 color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# 2D embedding of the digits dataset\n",
    "print(\"Computing embedding\")\n",
    "X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\n",
    "print(\"Done.\")\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "for linkage in ('ward', 'average', 'complete', 'single'):\n",
    "    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)\n",
    "    t0 = time()\n",
    "    clustering.fit(X_red)\n",
    "    print(\"%s :\\t%.2fs\" % (linkage, time() - t0))\n",
    "\n",
    "    plot_clustering(X_red, clustering.labels_, \"%s linkage\" % linkage)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=200000\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "vocabulary_size=len(tokenizer.word_index)+1\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector=en_model[word]\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinput1 = Input(shape=(length,))\n",
    "\tembedding_layer_1 = embedding_layer(input1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_layer_1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "\t# channel 2\n",
    "\tinput2 = Input(shape=(length,))\n",
    "\tembedding_layer_2 = embedding_layer(input2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding_layer_2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t# channel 3\n",
    "\tinput3 = Input(shape=(length,))\n",
    "\tembedding_layer_3 = embedding_layer(input3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding_layer_3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "\t# merge\n",
    "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "\t# LSTM\n",
    "\tx = embedding_layer(input3)\n",
    "\tLSTM_layer = LSTM(128)(x)\n",
    "\n",
    "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
    "    \n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
    "\toutputs = Dense(3, activation='softmax')(dense1)\n",
    "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    \n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\t# summarize\n",
    "\t#print(model.summary())\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     16090800    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 297, 32)      38432       embedding_1[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 295, 32)      57632       embedding_1[5][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 293, 32)      76832       embedding_1[6][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 297, 32)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 295, 32)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 293, 32)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 148, 32)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 147, 32)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 146, 32)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4736)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4704)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4672)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          219648      embedding_1[7][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14112)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14240)        0           lstm_2[0][0]                     \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           142410      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            33          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 16,625,787\n",
      "Trainable params: 534,987\n",
      "Non-trainable params: 16,090,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= define_model(EMBEDDING_DIM,vocabulary_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def toCategorical(y):\n",
    "    y = to_categorical(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = toCategorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 193556 samples, validate on 21507 samples\n",
      "Epoch 1/1\n",
      "193556/193556 [==============================] - 3824s 20ms/step - loss: 0.7614 - acc: 0.6850 - val_loss: 0.6952 - val_acc: 0.7203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da198b84a8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train,X_train,X_train], y_train, epochs=1, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cluster: 3\n",
      "Input shape: (215063, 300)\n",
      "Timestep: 215063\n",
      "Data dimension: 300\n"
     ]
    }
   ],
   "source": [
    "n_clusters = len(np.unique(y))\n",
    "print(\"Number of cluster: \" + str(n_clusters))\n",
    "print(\"Input shape: \" + str(X_train.shape))\n",
    "\n",
    "print(\"Timestep: \" + str(X_train.shape[0]))\n",
    "print(\"Data dimension: \" + str(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import objectives\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "def LSTM_AE(timesteps):\n",
    "    \"\"\"\n",
    "    Creates an LSTM Autoencoder (VAE). Returns Autoencoder, Encoder, Generator. \n",
    "    (All code by fchollet - see reference.)\n",
    "    # Arguments\n",
    "        input_dim: int.\n",
    "        timesteps: int, input timestep dimension.\n",
    "        latent_dim: int, latent z-layer shape. \n",
    "    # References\n",
    "        - [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=(EMBEDDING_DIM,))\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    encoded = LSTM(128, activation='relu',return_sequences=True)(x)\n",
    "    encoded = LSTM(64, activation='relu',return_sequences=False)(encoded)\n",
    "    \n",
    "    decoded = RepeatVector(timesteps)(encoded)    \n",
    "    decoded = LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = LSTM(128, activation='relu', return_sequences=True)(decoded)\n",
    "    decoded = TimeDistributed(Dense(EMBEDDING_DIM))(decoded)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=decoded, name='LSTM_AE'), Model(inputs=inputs, outputs=encoded, name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 300, 300)          16090800  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300, 128)          219648    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 1, 128)            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 300)            38700     \n",
      "=================================================================\n",
      "Total params: 16,530,396\n",
      "Trainable params: 439,596\n",
      "Non-trainable params: 16,090,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = LSTM_AE(1)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9f2adf859d9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m embedding_layer_v2 = Embedding(len(word_index) + 1,\n\u001b[0m\u001b[0;32m      4\u001b[0m                                 \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "embedding_layer_v2 = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=300,\n",
    "                                trainable=False)\n",
    "\n",
    "inputs = Input(shape=(EMBEDDING_DIM, ))\n",
    "embedded_sequences = embedding_layer_v2(inputs)\n",
    "encoded = LSTM(128)(embedded_sequences)\n",
    "bn = BatchNormalization()(encoded)\n",
    "do = Dropout(0.5)(bn)\n",
    "\n",
    "decoded = RepeatVector(300)(bn)\n",
    "decoded = LSTM(300)(decoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dropout(0.5)(decoded)\n",
    "\n",
    "#decoded = Dropout(0.5)(decoded)\n",
    "#decoded = Dense(300, activation='softmax')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps =  1 # equal to the lookback\n",
    "#autoencoder, encoder = LSTM_AE(timesteps)\n",
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_epochs = 1\n",
    "batch_size = 128\n",
    "save_dir = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   768/215063 [..............................] - ETA: 3:05:33 - loss: 5.7038"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d800d06d6b71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msequence_autoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.fit(X_train, y, batch_size=batch_size, epochs=pretrain_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid _j.\n",
    "                 q_ij = 1/(1+dist(x_i, _j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model.compile(optimizer='adam', loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.initializers import VarianceScaling\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn import mixture\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=5)\n",
    "bKmeans = DBSCAN()\n",
    "optics = OPTICS()\n",
    "gmm =  mixture.GaussianMixture()\n",
    "aggCluster = cluster.AgglomerativeClustering()\n",
    "spectral = cluster.SpectralClustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-0c018335cc82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbKmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_bKmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbKmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mSchubert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSander\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKriegel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mXu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mDBSCAN\u001b[0m \u001b[0mrevisited\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevisited\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhow\u001b[0m \u001b[0myou\u001b[0m \u001b[0mshould\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstill\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0muse\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mACM\u001b[0m \u001b[0mTransactions\u001b[0m \u001b[0mon\u001b[0m \u001b[0mDatabase\u001b[0m \u001b[0mSystems\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTODS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m19.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \"\"\"\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mis\u001b[0m \u001b[0moptimized\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0musage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m     \u001b[0mNotes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m     \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0mFor\u001b[0m \u001b[0man\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mplot_dbscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\cluster\\dbscan_.py\u001b[0m in \u001b[0;36mdbscan\u001b[1;34m(X, eps, min_samples, metric, metric_params, algorithm, leaf_size, p, sample_weight, n_jobs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# Calculate neighborhood for all samples. This leaves the original point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mradius_neighbors\u001b[1;34m(self, X, radius, return_distance)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mpoint\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mlying\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mball\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0msize\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0maround\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mPoints\u001b[0m \u001b[0mlying\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mboundary\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincluded\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0mare\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;32mnot\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mnecessarily\u001b[0m \u001b[0msorted\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtheir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\neighbors\\binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.kd_tree.BinaryTree.query_radius\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred_kmeans = kmeans.fit_predict(encoder.predict(X_train))\n",
    "y_pred_bKmeans = bKmeans.fit_predict(encoder.predict(X_train))\n",
    "y_pred_optics = optics.fit_predict(encoder.predict(X_train))\n",
    "y_pred_gmm = gmm.fit_predict(encoder.predict(X_train))\n",
    "y_pred_aggCluster = aggCluster.fit_predict(encoder.predict(X_train))\n",
    "y_pred_spectral = spectral.fit_predict(encoder.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033115877673054"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.acc(y, y_pred_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, y_pred_bKmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 10000\n",
    "update_interval = 10\n",
    "index_array = np.arange(X_train.shape[0])\n",
    "tol = 0.01 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:605: RuntimeWarning: invalid value encountered in log\n",
      "  A clustering of the data into disjoint subsets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: acc = 0.24903, nmi = nan, ari = 0.00002  ; loss= 0.0\n",
      "Iter 10: acc = 0.24903, nmi = nan, ari = 0.00002  ; loss= 1e-05\n",
      "delta_label  0.0 < tol  0.01\n",
      "Reached tolerance threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(X_train, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(accuracy_score(y, y_pred), 5)\n",
    "            nmi = np.round(mutual_info_score(y, y_pred), 5)\n",
    "            ari = np.round(adjusted_rand_score(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, X_train.shape[0])]\n",
    "    loss = model.train_on_batch(x=X_train[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= X_train.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/conv_DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(save_dir + '/conv_DEC_model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale=3)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_data(x_train_encoded, y_train, vis_dim, n_predict, n_train, build_anim):\n",
    "    cmap = plt.get_cmap('rainbow', 10)\n",
    "\n",
    "    # 3-dim vis: show one view, then compile animated .gif of many angled views\n",
    "    if vis_dim == 3:\n",
    "        # Simple static figure\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes(projection='3d')\n",
    "        p = ax.scatter3D(x_train_encoded[:,0], x_train_encoded[:,1], x_train_encoded[:,2], \n",
    "                c=y_train[:n_predict], cmap=cmap, edgecolor='black')\n",
    "        fig.colorbar(p, drawedges=True)\n",
    "        plt.show()\n",
    "\n",
    "        # Build animation from many static figures\n",
    "        if build_anim:\n",
    "            angles = np.linspace(180, 360, 20)\n",
    "            i = 0\n",
    "            for angle in angles:\n",
    "                fig = plt.figure()\n",
    "                ax = plt.axes(projection='3d')\n",
    "                ax.view_init(10, angle)\n",
    "                p = ax.scatter3D(x_train_encoded[:,0], x_train_encoded[:,1], x_train_encoded[:,2], \n",
    "                        c=y_train[:n_predict], cmap=cmap, edgecolor='black')\n",
    "                fig.colorbar(p, drawedges=True)\n",
    "                outfile = 'anim/3dplot_step_' + chr(i + 97) + '.png'\n",
    "                plt.savefig(outfile, dpi=96)\n",
    "                i += 1\n",
    "            call(['convert', '-delay', '50', 'anim/3dplot*', 'anim/3dplot_anim_' + str(n_train) + '.gif'])\n",
    "\n",
    "    # 2-dim vis: plot and colorbar.\n",
    "    elif vis_dim == 2:\n",
    "        plt.scatter(x_train_encoded[:,0], x_train_encoded[:,1], \n",
    "                c=y_train[:n_predict], edgecolor='black', cmap=cmap)\n",
    "        plt.colorbar(drawedges=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a number of MNIST digits, then perform t-SNE dim-reduction.\n",
    "x_train_predict = encoder.predict(data)\n",
    "\n",
    "#print \"Performing t-SNE dimensionality reduction...\"\n",
    "x_train_encoded = TSNE(n_components=2).fit_transform(x_train_predict)\n",
    "#np.save('%sx_%sdim_tnse_%s.npy' % (266, 2, 266), x_train_encoded)\n",
    "#x_train_encoded = np.load(str(n_predict) + 'x_' + str(vis_dim) + 'dim_tnse_' + str(n_train) + '.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize result.\n",
    "train_new = False\n",
    "n_train = 70000\n",
    "predict_new = False\n",
    "n_predict = 70000\n",
    "vis_dim = 2\n",
    "build_anim = False\n",
    "    \n",
    "vis_data(x_train_encoded, y, vis_dim, n_predict, n_train, build_anim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
